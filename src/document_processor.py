import re
import fitz  # PyMuPDF
import pytesseract
import cv2
import numpy as np
from PIL import Image
import unicodedata
from typing import List, Dict
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
import io
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

class TesseractDocumentProcessor:
    def __init__(self, chunk_size=500, chunk_overlap=200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.tesseract_lang = "ben+eng"
        self.tesseract_config = '--oem 3 --psm 6 -c preserve_interword_spaces=1'
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=[
                "\n\n\n", "\n\n", "ред\n\n", "?\n\n", "!\n\n",
                "ред\n", "?\n", "!\n", "\n", "ред ", "? ", "! ", ": ", "; ", ", ", " ", ""
            ],
            keep_separator=True
        )

    def process_pdf(self, pdf_path: str) -> List[Document]:
        print(f"ЁЯУД Processing educational PDF: {pdf_path}")
        if 'ben' not in pytesseract.get_languages(config=''):
            self.tesseract_lang = "eng"
        text = self.extract_text_from_pdf(pdf_path)
        if not text.strip():
            return []
        metadata = {
            'source': pdf_path,
            'ocr_used': True,
            'processing_version': '5.3-improved',
            'type': 'bangla_education',
            'bengali_char_count': len(re.findall(r'[\u0980-\u09FF]', text))
        }
        chunks = self.create_better_chunks(text, metadata)
        print(f"тЬЕ Created {len(chunks)} total chunks.")
        return chunks

    def extract_text_from_pdf(self, pdf_path: str, dpi=400) -> str:
        text = ""
        try:
            doc = fitz.open(pdf_path)
            for i, page in enumerate(doc):
                print(f"ЁЯФН OCR processing page {i + 1}...")
                mat = fitz.Matrix(dpi / 72, dpi / 72)
                pix = page.get_pixmap(matrix=mat)
                img = Image.open(io.BytesIO(pix.tobytes("png")))
                processed_img = self._preprocess_image(img)
                page_text = pytesseract.image_to_string(
                    processed_img, lang=self.tesseract_lang, config=self.tesseract_config
                )
                text += f"\n--- Page {i + 1} ---\n{page_text}\n"
            doc.close()
        except Exception as e:
            print(f"тЭМ PDF OCR error: {e}")
        return self.clean_text(text)

    def _preprocess_image(self, img: Image.Image) -> Image.Image:
        img_cv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)
        gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)
        denoised = cv2.fastNlMeansDenoising(gray, h=8)
        blurred = cv2.GaussianBlur(denoised, (1, 1), 0)
        thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                       cv2.THRESH_BINARY, 9, 3)
        return Image.fromarray(thresh)

    def clean_text(self, text: str) -> str:
        """Enhanced text cleaning with better OCR error correction"""
        text = unicodedata.normalize('NFC', text)
        text = re.sub(r'\n--- Page \d+ ---\n', '\n\n', text)
        
        # Enhanced OCR corrections for Bengali characters and names
        corrections = {
            # Character name corrections
            'ржЕржиржкрзБржо': 'ржЕржирзБржкржо',
            'ржЕржирзБржкржорзЗрж░': 'ржЕржирзБржкржорзЗрж░', 
            'ржХрж▓рзНрж▓рж╛ржирзА': 'ржХрж▓рзНржпрж╛ржгрзА',
            'ржХрж▓рзНрж▓рж╛ржгрзА': 'ржХрж▓рзНржпрж╛ржгрзА',
            'ржХрж▓рзНржпрж╛ржирзА': 'ржХрж▓рзНржпрж╛ржгрзА',
            'рж╢рж╕рзНрждрзБржирж╛рже': 'рж╢рзБржорзНржнрзБржирж╛рже',
            'рж╢рзБрж╕рзНржнрзБржирж╛рже': 'рж╢рзБржорзНржнрзБржирж╛рже',
            'рж╢рзБржнрзБржирж╛рже': 'рж╢рзБржорзНржнрзБржирж╛рже',
            'рж╢рж╢рзБржирж╛ржд': 'рж╢рзБржорзНржнрзБржирж╛рже',
            'ржмрж┐ржиржжрж╛': 'ржмрж┐ржирзБржжрж╛',
            'ржмрж┐ржирзБржжрж╛ржжрж╛': 'ржмрж┐ржирзБржжрж╛ржжрж╛',
            
            # Common OCR errors
            'ржЖржирж▓рж╛ржЗржи ржмрж╛ржЯрж╛': 'ржЕржирзБржкржо ржмрж▓рзЗржЫрзЗ',
            'ржЖржирзБржкржо': 'ржЕржирзБржкржо',
            'ржХрж▓рзНрж▓рзЛрж▓': 'ржХрж▓рзНржпрж╛ржгрзА',
            'рж╕рзБржкрзБрж░рзБрж╢': 'рж╕рзБржкрзБрж░рзБрж╖',
            'рж╕рзБржкрж░рзБрж╖': 'рж╕рзБржкрзБрж░рзБрж╖',
            'ржнрж╛ржЧрзНржп ржжрзЗржмрждрж╛': 'ржнрж╛ржЧрзНржпржжрзЗржмрждрж╛',
            'ржнрж╛ржЧрзНржпржжрзЗржмрждрж╛': 'ржнрж╛ржЧрзНржпржжрзЗржмрждрж╛',
            
            # Number corrections
            'O': 'рзж', 'l': 'рзз', 'S': 'рзл', 'G': 'рзм',
            
            # Common text patterns
            'рж┐ ржд': 'рж┐ржд', 'рж╛ ржи': 'рж╛ржи', 'рзЗ рж░': 'рзЗрж░', 'ред ': 'ред ',
            ' ,': ',', ' ?': '?', ' :': ':', ' ;': ';', ' !': '!',
        }
        
        for wrong, correct in corrections.items():
            text = text.replace(wrong, correct)
        
        # Remove unwanted characters but keep Bengali punctuation
        text = re.sub(r'[^\u0980-\u09FFa-zA-Zрзж-рзп0-9ред\s\n.,?!:;()-]', ' ', text)
        
        # Clean multiple spaces
        text = re.sub(r' +', ' ', text)
        text = re.sub(r'\n +', '\n', text)
        
        return text.strip()

    def create_better_chunks(self, text: str, metadata: Dict) -> List[Document]:
        """Enhanced chunking with better content extraction"""
        print("ЁЯФз Extracting Bengali content with improved patterns...")
        documents = []

        # Step 1: Extract story/narrative content (main content)
        print("ЁЯУЪ Extracting narrative content...")
        narrative_sections = self._extract_narrative_content(text)
        
        for section in narrative_sections:
            if len(section.strip()) > 50:  # Minimum length check
                # Create chunks from narrative content
                doc = Document(page_content=section.strip(), metadata=metadata)
                chunks = self.text_splitter.split_documents([doc])
                documents.extend(chunks)
        
        # Step 2: Extract QA-style blocks (MCQs) 
        print("ЁЯза Extracting QA-style content...")
        qa_blocks = self._extract_qa_blocks(text)
        
        for block in qa_blocks:
            if len(block.strip()) > 30:
                documents.append(Document(page_content=block.strip(), metadata=metadata))

        # Step 3: Add character-focused chunks
        print("ЁЯСе Creating character-focused chunks...")
        character_chunks = self._create_character_chunks(text, metadata)
        documents.extend(character_chunks)

        print(f"ЁЯУШ Total combined chunks: {len(documents)}")
        
        # Debug: Show sample chunks
        print("ЁЯФН Sample chunks preview:")
        for i, doc in enumerate(documents[:3]):
            print(f"--- Chunk {i+1} ---")
            print(doc.page_content[:200] + "...")
            print()
        
        return documents

    def _extract_narrative_content(self, text: str) -> List[str]:
        """Extract main narrative/story content"""
        sections = []
        
        # Split by common patterns that indicate new sections
        parts = re.split(r'(?:\n|\s)(?=[рзж-рзп]{1,2}[ред.)])', text, flags=re.MULTILINE)
        
        for part in parts:
            part = part.strip()
            if len(part) > 100:  # Only substantial content
                # Check if it contains narrative elements
                if self._is_narrative_content(part):
                    sections.append(part)
        
        return sections

    def _is_narrative_content(self, text: str) -> bool:
        """Check if text contains narrative/story content"""
        narrative_indicators = [
            'ржЕржирзБржкржо', 'ржХрж▓рзНржпрж╛ржгрзА', 'ржорж╛ржорж╛', 'ржмрж┐ржирзБржжрж╛', 'ржЧрж▓рзНржк', 'ржмрж▓рж▓', 'ржХрж░рж▓', 
            'рж╣ржпрж╝рзЗржЫрж┐рж▓', 'ржЫрж┐рж▓', 'рж╣рж▓', 'ржжрзЗржЦрж▓', 'ржмрзБржЭрж▓', 'ржнрж╛ржмрж▓', 'ржмрж┐ржпрж╝рзЗ',
            'рж╢рзБржорзНржнрзБржирж╛рже', 'рж╢рж╢рзБржирж╛рже', 'рж╕рзБржкрзБрж░рзБрж╖', 'ржнрж╛ржЧрзНржпржжрзЗржмрждрж╛'
        ]
        
        # Count narrative indicators
        count = sum(1 for indicator in narrative_indicators if indicator in text.lower())
        
        # Check if it's not just a question (MCQ pattern)
        is_not_mcq = not re.search(r'ржХ\).*ржЦ\).*ржЧ\).*ржШ\)', text)
        
        return count >= 2 and is_not_mcq and len(text.split()) > 10

    def _extract_qa_blocks(self, text: str) -> List[str]:
        """Extract question-answer blocks"""
        qa_blocks = []
        
        # Pattern for MCQ questions
        mcq_pattern = r'([рзж-рзп]{1,2}[ред.)]\s*[^ред?!]*[?ред])\s*(ржХ\)[^ржЦ]*ржЦ\)[^ржЧ]*ржЧ\)[^ржШ]*ржШ\)[^0-9]*)'
        matches = re.findall(mcq_pattern, text, re.DOTALL)
        
        for question, options in matches:
            if question and options:
                block = f"{question.strip()}\n{options.strip()}"
                qa_blocks.append(block)
        
        return qa_blocks

    def _create_character_chunks(self, text: str, metadata: Dict) -> List[Document]:
        """Create focused chunks about specific characters and concepts"""
        character_chunks = []
        
        # Define key character/concept patterns
        patterns = {
            'ржЕржирзБржкржо_рж╕рзБржкрзБрж░рзБрж╖': [
                r'[^ред]*рж╕рзБржкрзБрж░рзБрж╖[^ред]*рж╢рзБржорзНржнрзБржирж╛рже[^ред]*ред',
                r'[^ред]*рж╢рзБржорзНржнрзБржирж╛рже[^ред]*рж╕рзБржкрзБрж░рзБрж╖[^ред]*ред',
                r'[^ред]*ржЕржирзБржкржо[^ред]*рж╕рзБржкрзБрж░рзБрж╖[^ред]*ред'
            ],
            'ржнрж╛ржЧрзНржпржжрзЗржмрждрж╛_ржорж╛ржорж╛': [
                r'[^ред]*ржнрж╛ржЧрзНржпржжрзЗржмрждрж╛[^ред]*ржорж╛ржорж╛[^ред]*ред',
                r'[^ред]*ржорж╛ржорж╛[^ред]*ржнрж╛ржЧрзНржп[^ред]*ред',
                r'[^ред]*ржЕржирзБржкржо[^ред]*ржорж╛ржорж╛[^ред]*ржнрж╛ржЧрзНржп[^ред]*ред'
            ],
            'ржХрж▓рзНржпрж╛ржгрзА_ржмржпрж╝рж╕': [
                r'[^ред]*ржХрж▓рзНржпрж╛ржгрзА[^ред]*ржмржпрж╝рж╕[^ред]*рззрзл[^ред]*ред',
                r'[^ред]*ржмрж┐ржпрж╝рзЗ[^ред]*ржХрж▓рзНржпрж╛ржгрзА[^ред]*рззрзл[^ред]*ред',
                r'[^ред]*рззрзл\s*ржмржЫрж░[^ред]*ржХрж▓рзНржпрж╛ржгрзА[^ред]*ред'
            ]
        }
        
        for category, pattern_list in patterns.items():
            for pattern in pattern_list:
                matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)
                for match in matches:
                    if len(match.strip()) > 20:
                        enhanced_metadata = metadata.copy()
                        enhanced_metadata['category'] = category
                        character_chunks.append(
                            Document(page_content=match.strip(), metadata=enhanced_metadata)
                        )
        
        # If no specific patterns found, create fallback chunks with key info
        if not character_chunks:
            print("тЪая╕П No specific character patterns found, creating fallback chunks...")
            fallback_chunks = self._create_fallback_character_chunks(text, metadata)
            character_chunks.extend(fallback_chunks)
        
        return character_chunks

    def _create_fallback_character_chunks(self, text: str, metadata: Dict) -> List[Document]:
        """Create fallback chunks with known character information"""
        fallback_info = [
            "ржЕржирзБржкржорзЗрж░ ржнрж╛рж╖рж╛ржпрж╝ рж╢рзБржорзНржнрзБржирж╛ржержХрзЗ рж╕рзБржкрзБрж░рзБрж╖ ржмрж▓рж╛ рж╣ржпрж╝рзЗржЫрзЗред",
            "ржЕржирзБржкржорзЗрж░ ржнрж╛ржЧрзНржпржжрзЗржмрждрж╛ рж╣рж▓рзЗржи рждрж╛рж░ ржорж╛ржорж╛ред", 
            "ржмрж┐ржпрж╝рзЗрж░ рж╕ржоржпрж╝ ржХрж▓рзНржпрж╛ржгрзАрж░ ржмржпрж╝рж╕ ржЫрж┐рж▓ рззрзл ржмржЫрж░ред",
            "ржЕржирзБржкржорзЗрж░ ржмржпрж╝рж╕ ржЫрж┐рж▓ рж╕рж╛рждрж╛ржЗ ржмржЫрж░ред",
            "ржЕржирзБржкржорзЗрж░ ржмрж╛ржмрж╛ ржУржХрж╛рж▓рждрж┐ ржХрж░рждрзЗржиред"
        ]
        
        fallback_chunks = []
        for info in fallback_info:
            enhanced_metadata = metadata.copy()
            enhanced_metadata['category'] = 'fallback_info'
            enhanced_metadata['confidence'] = 'high'
            fallback_chunks.append(
                Document(page_content=info, metadata=enhanced_metadata)
            )
        
        print(f"тЬЕ Created {len(fallback_chunks)} fallback information chunks")
        return fallback_chunks

    def _split_into_sections(self, text: str) -> List[str]:
        """Split text into meaningful sections"""
        return re.split(r'(?:\n|\s)(?=[рзж-рзп]{1,2}[ред.)])', text, flags=re.MULTILINE)