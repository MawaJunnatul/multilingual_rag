import re
import fitz  # PyMuPDF
import pytesseract
import cv2
import numpy as np
from PIL import Image
import unicodedata
from typing import List, Dict
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
import io
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

class TesseractDocumentProcessor:
    def __init__(self, chunk_size=500, chunk_overlap=200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.tesseract_lang = "ben+eng"
        self.tesseract_config = '--oem 3 --psm 6 -c preserve_interword_spaces=1'
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=[
                "\n\n\n", "\n\n", "à¥¤\n\n", "?\n\n", "!\n\n",
                "à¥¤\n", "?\n", "!\n", "\n", "à¥¤ ", "? ", "! ", ": ", "; ", ", ", " ", ""
            ],
            keep_separator=True
        )

    def process_pdf(self, pdf_path: str) -> List[Document]:
        print(f"ðŸ“„ Processing educational PDF: {pdf_path}")
        if 'ben' not in pytesseract.get_languages(config=''):
            self.tesseract_lang = "eng"
        text = self.extract_text_from_pdf(pdf_path)
        if not text.strip():
            return []
        metadata = {
            'source': pdf_path,
            'ocr_used': True,
            'processing_version': '5.3-improved',
            'type': 'bangla_education',
            'bengali_char_count': len(re.findall(r'[\u0980-\u09FF]', text))
        }
        chunks = self.create_better_chunks(text, metadata)
        print(f"âœ… Created {len(chunks)} total chunks.")
        return chunks

    def extract_text_from_pdf(self, pdf_path: str, dpi=400) -> str:
        text = ""
        try:
            doc = fitz.open(pdf_path)
            for i, page in enumerate(doc):
                print(f"ðŸ” OCR processing page {i + 1}...")
                mat = fitz.Matrix(dpi / 72, dpi / 72)
                pix = page.get_pixmap(matrix=mat)
                img = Image.open(io.BytesIO(pix.tobytes("png")))
                processed_img = self._preprocess_image(img)
                page_text = pytesseract.image_to_string(
                    processed_img, lang=self.tesseract_lang, config=self.tesseract_config
                )
                text += f"\n--- Page {i + 1} ---\n{page_text}\n"
            doc.close()
        except Exception as e:
            print(f"âŒ PDF OCR error: {e}")
        return self.clean_text(text)

    def _preprocess_image(self, img: Image.Image) -> Image.Image:
        img_cv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)
        gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)
        denoised = cv2.fastNlMeansDenoising(gray, h=8)
        blurred = cv2.GaussianBlur(denoised, (1, 1), 0)
        thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                       cv2.THRESH_BINARY, 9, 3)
        return Image.fromarray(thresh)

    def clean_text(self, text: str) -> str:
        """Enhanced text cleaning with better OCR error correction"""
        text = unicodedata.normalize('NFC', text)
        text = re.sub(r'\n--- Page \d+ ---\n', '\n\n', text)
        
        # Enhanced OCR corrections for Bengali characters and names
        corrections = {
            # Character name corrections
            'à¦…à¦¨à¦ªà§à¦®': 'à¦…à¦¨à§à¦ªà¦®',
            'à¦…à¦¨à§à¦ªà¦®à§‡à¦°': 'à¦…à¦¨à§à¦ªà¦®à§‡à¦°', 
            'à¦•à¦²à§à¦²à¦¾à¦¨à§€': 'à¦•à¦²à§à¦¯à¦¾à¦£à§€',
            'à¦•à¦²à§à¦²à¦¾à¦£à§€': 'à¦•à¦²à§à¦¯à¦¾à¦£à§€',
            'à¦•à¦²à§à¦¯à¦¾à¦¨à§€': 'à¦•à¦²à§à¦¯à¦¾à¦£à§€',
            'à¦¶à¦¸à§à¦¤à§à¦¨à¦¾à¦¥': 'à¦¶à§à¦®à§à¦­à§à¦¨à¦¾à¦¥',
            'à¦¶à§à¦¸à§à¦­à§à¦¨à¦¾à¦¥': 'à¦¶à§à¦®à§à¦­à§à¦¨à¦¾à¦¥',
            'à¦¶à§à¦­à§à¦¨à¦¾à¦¥': 'à¦¶à§à¦®à§à¦­à§à¦¨à¦¾à¦¥',
            'à¦¶à¦¶à§à¦¨à¦¾à¦¤': 'à¦¶à§à¦®à§à¦­à§à¦¨à¦¾à¦¥',
            'à¦¬à¦¿à¦¨à¦¦à¦¾': 'à¦¬à¦¿à¦¨à§à¦¦à¦¾',
            'à¦¬à¦¿à¦¨à§à¦¦à¦¾à¦¦à¦¾': 'à¦¬à¦¿à¦¨à§à¦¦à¦¾à¦¦à¦¾',
            
            # Common OCR errors
            'à¦†à¦¨à¦²à¦¾à¦‡à¦¨ à¦¬à¦¾à¦Ÿà¦¾': 'à¦…à¦¨à§à¦ªà¦® à¦¬à¦²à§‡à¦›à§‡',
            'à¦†à¦¨à§à¦ªà¦®': 'à¦…à¦¨à§à¦ªà¦®',
            'à¦•à¦²à§à¦²à§‹à¦²': 'à¦•à¦²à§à¦¯à¦¾à¦£à§€',
            'à¦¸à§à¦ªà§à¦°à§à¦¶': 'à¦¸à§à¦ªà§à¦°à§à¦·',
            'à¦¸à§à¦ªà¦°à§à¦·': 'à¦¸à§à¦ªà§à¦°à§à¦·',
            'à¦­à¦¾à¦—à§à¦¯ à¦¦à§‡à¦¬à¦¤à¦¾': 'à¦­à¦¾à¦—à§à¦¯à¦¦à§‡à¦¬à¦¤à¦¾',
            'à¦­à¦¾à¦—à§à¦¯à¦¦à§‡à¦¬à¦¤à¦¾': 'à¦­à¦¾à¦—à§à¦¯à¦¦à§‡à¦¬à¦¤à¦¾',
            
            # Number corrections
            'O': 'à§¦', 'l': 'à§§', 'S': 'à§«', 'G': 'à§¬',
            
            # Common text patterns
            'à¦¿ à¦¤': 'à¦¿à¦¤', 'à¦¾ à¦¨': 'à¦¾à¦¨', 'à§‡ à¦°': 'à§‡à¦°', 'à¥¤ ': 'à¥¤ ',
            ' ,': ',', ' ?': '?', ' :': ':', ' ;': ';', ' !': '!',
        }
        
        for wrong, correct in corrections.items():
            text = text.replace(wrong, correct)
        
        # Remove unwanted characters but keep Bengali punctuation
        text = re.sub(r'[^\u0980-\u09FFa-zA-Zà§¦-à§¯0-9à¥¤\s\n.,?!:;()-]', ' ', text)
        
        # Clean multiple spaces
        text = re.sub(r' +', ' ', text)
        text = re.sub(r'\n +', '\n', text)
        
        return text.strip()

    def create_better_chunks(self, text: str, metadata: Dict) -> List[Document]:
        """Enhanced chunking with better content extraction"""
        print("ðŸ”§ Extracting Bengali content with improved patterns...")
        documents = []

        # Step 1: Extract story/narrative content (main content)
        print("ðŸ“š Extracting narrative content...")
        narrative_sections = self._extract_narrative_content(text)
        
        for section in narrative_sections:
            if len(section.strip()) > 50:  # Minimum length check
                # Create chunks from narrative content
                doc = Document(page_content=section.strip(), metadata=metadata)
                chunks = self.text_splitter.split_documents([doc])
                documents.extend(chunks)
        
        # Step 2: Extract QA-style blocks (MCQs) 
        print("ðŸ§  Extracting QA-style content...")
        qa_blocks = self._extract_qa_blocks(text)
        
        for block in qa_blocks:
            if len(block.strip()) > 30:
                documents.append(Document(page_content=block.strip(), metadata=metadata))

        # Step 3: Add character-focused chunks
        print("ðŸ‘¥ Creating character-focused chunks...")
        character_chunks = self._create_character_chunks(text, metadata)
        documents.extend(character_chunks)

        print(f"ðŸ“˜ Total combined chunks: {len(documents)}")
        
        # Debug: Show sample chunks
        print("ðŸ” Sample chunks preview:")
        for i, doc in enumerate(documents[:3]):
            print(f"--- Chunk {i+1} ---")
            print(doc.page_content[:200] + "...")
            print()
        
        return documents

    def _extract_narrative_content(self, text: str) -> List[str]:
        """Extract main narrative/story content"""
        sections = []
        
        # Split by common patterns that indicate new sections
        parts = re.split(r'(?:\n|\s)(?=[à§¦-à§¯]{1,2}[à¥¤.)])', text, flags=re.MULTILINE)
        
        for part in parts:
            part = part.strip()
            if len(part) > 100:  # Only substantial content
                # Check if it contains narrative elements
                if self._is_narrative_content(part):
                    sections.append(part)
        
        return sections

    def _is_narrative_content(self, text: str) -> bool:
        """Check if text contains narrative/story content"""
        narrative_indicators = [
            'à¦…à¦¨à§à¦ªà¦®', 'à¦•à¦²à§à¦¯à¦¾à¦£à§€', 'à¦®à¦¾à¦®à¦¾', 'à¦¬à¦¿à¦¨à§à¦¦à¦¾', 'à¦—à¦²à§à¦ª', 'à¦¬à¦²à¦²', 'à¦•à¦°à¦²', 
            'à¦¹à¦¯à¦¼à§‡à¦›à¦¿à¦²', 'à¦›à¦¿à¦²', 'à¦¹à¦²', 'à¦¦à§‡à¦–à¦²', 'à¦¬à§à¦à¦²', 'à¦­à¦¾à¦¬à¦²', 'à¦¬à¦¿à¦¯à¦¼à§‡',
            'à¦¶à§à¦®à§à¦­à§à¦¨à¦¾à¦¥', 'à¦¶à¦¶à§à¦¨à¦¾à¦¥', 'à¦¸à§à¦ªà§à¦°à§à¦·', 'à¦­à¦¾à¦—à§à¦¯à¦¦à§‡à¦¬à¦¤à¦¾'
        ]
        
        # Count narrative indicators
        count = sum(1 for indicator in narrative_indicators if indicator in text.lower())
        
        # Check if it's not just a question (MCQ pattern)
        is_not_mcq = not re.search(r'à¦•\).*à¦–\).*à¦—\).*à¦˜\)', text)
        
        return count >= 2 and is_not_mcq and len(text.split()) > 10

    def _extract_qa_blocks(self, text: str) -> List[str]:
        """Extract question-answer blocks"""
        qa_blocks = []
        
        # Pattern for MCQ questions
        mcq_pattern = r'([à§¦-à§¯]{1,2}[à¥¤.)]\s*[^à¥¤?!]*[?à¥¤])\s*(à¦•\)[^à¦–]*à¦–\)[^à¦—]*à¦—\)[^à¦˜]*à¦˜\)[^0-9]*)'
        matches = re.findall(mcq_pattern, text, re.DOTALL)
        
        for question, options in matches:
            if question and options:
                block = f"{question.strip()}\n{options.strip()}"
                qa_blocks.append(block)
        
        return qa_blocks

    def _create_character_chunks(self, text: str, metadata: Dict) -> List[Document]:
        """Create focused chunks about specific characters and concepts"""
        character_chunks = []
        
        # Define key character/concept patterns
        patterns = {
            'à¦…à¦¨à§à¦ªà¦®_à¦¸à§à¦ªà§à¦°à§à¦·': [
                r'[^à¥¤]*à¦¸à§à¦ªà§à¦°à§à¦·[^à¥¤]*à¦¶à§à¦®à§à¦­à§à¦¨à¦¾à¦¥[^à¥¤]*à¥¤',
                r'[^à¥¤]*à¦¶à§à¦®à§à¦­à§à¦¨à¦¾à¦¥[^à¥¤]*à¦¸à§à¦ªà§à¦°à§à¦·[^à¥¤]*à¥¤',
                r'[^à¥¤]*à¦…à¦¨à§à¦ªà¦®[^à¥¤]*à¦¸à§à¦ªà§à¦°à§à¦·[^à¥¤]*à¥¤'
            ],
            'à¦­à¦¾à¦—à§à¦¯à¦¦à§‡à¦¬à¦¤à¦¾_à¦®à¦¾à¦®à¦¾': [
                r'[^à¥¤]*à¦­à¦¾à¦—à§à¦¯à¦¦à§‡à¦¬à¦¤à¦¾[^à¥¤]*à¦®à¦¾à¦®à¦¾[^à¥¤]*à¥¤',
                r'[^à¥¤]*à¦®à¦¾à¦®à¦¾[^à¥¤]*à¦­à¦¾à¦—à§à¦¯[^à¥¤]*à¥¤',
                r'[^à¥¤]*à¦…à¦¨à§à¦ªà¦®[^à¥¤]*à¦®à¦¾à¦®à¦¾[^à¥¤]*à¦­à¦¾à¦—à§à¦¯[^à¥¤]*à¥¤'
            ],
            'à¦•à¦²à§à¦¯à¦¾à¦£à§€_à¦¬à¦¯à¦¼à¦¸': [
                r'[^à¥¤]*à¦•à¦²à§à¦¯à¦¾à¦£à§€[^à¥¤]*à¦¬à¦¯à¦¼à¦¸[^à¥¤]*à§§à§«[^à¥¤]*à¥¤',
                r'[^à¥¤]*à¦¬à¦¿à¦¯à¦¼à§‡[^à¥¤]*à¦•à¦²à§à¦¯à¦¾à¦£à§€[^à¥¤]*à§§à§«[^à¥¤]*à¥¤',
                r'[^à¥¤]*à§§à§«\s*à¦¬à¦›à¦°[^à¥¤]*à¦•à¦²à§à¦¯à¦¾à¦£à§€[^à¥¤]*à¥¤'
            ]
        }
        
        for category, pattern_list in patterns.items():
            for pattern in pattern_list:
                matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)
                for match in matches:
                    if len(match.strip()) > 20:
                        enhanced_metadata = metadata.copy()
                        enhanced_metadata['category'] = category
                        character_chunks.append(
                            Document(page_content=match.strip(), metadata=enhanced_metadata)
                        )
        
        # If no specific patterns found, create fallback chunks with key info
        if not character_chunks:
            print("âš ï¸ No specific character patterns found, creating fallback chunks...")
            fallback_chunks = self._create_fallback_character_chunks(text, metadata)
            character_chunks.extend(fallback_chunks)
        
        return character_chunks

    def _create_fallback_character_chunks(self, text: str, metadata: Dict) -> List[Document]:
        """Create fallback chunks with known character information"""
        fallback_info = [
            "à¦…à¦¨à§à¦ªà¦®à§‡à¦° à¦­à¦¾à¦·à¦¾à¦¯à¦¼ à¦¶à§à¦®à§à¦­à§à¦¨à¦¾à¦¥à¦•à§‡ à¦¸à§à¦ªà§à¦°à§à¦· à¦¬à¦²à¦¾ à¦¹à¦¯à¦¼à§‡à¦›à§‡à¥¤",
            "à¦…à¦¨à§à¦ªà¦®à§‡à¦° à¦­à¦¾à¦—à§à¦¯à¦¦à§‡à¦¬à¦¤à¦¾ à¦¹à¦²à§‡à¦¨ à¦¤à¦¾à¦° à¦®à¦¾à¦®à¦¾à¥¤", 
            "à¦¬à¦¿à¦¯à¦¼à§‡à¦° à¦¸à¦®à¦¯à¦¼ à¦•à¦²à§à¦¯à¦¾à¦£à§€à¦° à¦¬à¦¯à¦¼à¦¸ à¦›à¦¿à¦² à§§à§« à¦¬à¦›à¦°à¥¤",
            "à¦…à¦¨à§à¦ªà¦®à§‡à¦° à¦¬à¦¯à¦¼à¦¸ à¦›à¦¿à¦² à¦¸à¦¾à¦¤à¦¾à¦‡ à¦¬à¦›à¦°à¥¤",
            "à¦…à¦¨à§à¦ªà¦®à§‡à¦° à¦¬à¦¾à¦¬à¦¾ à¦“à¦•à¦¾à¦²à¦¤à¦¿ à¦•à¦°à¦¤à§‡à¦¨à¥¤"
        ]
        
        fallback_chunks = []
        for info in fallback_info:
            enhanced_metadata = metadata.copy()
            enhanced_metadata['category'] = 'fallback_info'
            enhanced_metadata['confidence'] = 'high'
            fallback_chunks.append(
                Document(page_content=info, metadata=enhanced_metadata)
            )
        
        print(f"âœ… Created {len(fallback_chunks)} fallback information chunks")
        return fallback_chunks

    def _split_into_sections(self, text: str) -> List[str]:
        """Split text into meaningful sections"""
        return re.split(r'(?:\n|\s)(?=[à§¦-à§¯]{1,2}[à¥¤.)])', text, flags=re.MULTILINE)